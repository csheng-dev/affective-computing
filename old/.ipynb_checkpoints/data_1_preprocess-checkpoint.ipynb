{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "79d58431-71ca-4ede-9441-193c896988c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "81c7fd20-719a-4fba-8c3a-51ec3f77b2cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AEROBIC\n",
      "ANAEROBIC\n",
      "STRESS\n",
      "wearable-device-dataset-from-induced-stress-and-structured-exercise-sessions-1.0.0\n"
     ]
    }
   ],
   "source": [
    "# Parameters\n",
    "fs = 32              # Sampling frequency (Hz)\n",
    "chunk_duration = 4   # seconds\n",
    "overlap_duration = 1 # seconds\n",
    "root_path = '/Users/sheng/Documents/emotion_model_project/data'\n",
    "\n",
    "# Set the path to the target directory\n",
    "base_path = root_path + '/1'\n",
    "\n",
    "# Get a list of all subdirectories in \"1\"\n",
    "subdirs = [name for name in os.listdir(base_path) if os.path.isdir(os.path.join(base_path, name))]\n",
    "\n",
    "# Print each subdirectory name\n",
    "for subdir in subdirs:\n",
    "    print(subdir)\n",
    "    \n",
    "chunks_all = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0a04b6c-f138-40a1-bc50-1f5d33b1c7c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e14a16f4-bc00-441f-b58b-8211c1459e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(3):\n",
    "    dir_name = subdirs[i]\n",
    "    dir_path = os.path.join(base_path, dir_name)\n",
    "    subject_dirs = [name for name in os.listdir(dir_path) if os.path.isdir(os.path.join(dir_path, name))]\n",
    "    subject_dirs\n",
    "    for j in range(len(subject_dirs)):\n",
    "        subject_dirs[j]\n",
    "        os.listdir(os.path.join(dir_path, subject_dirs[j]))\n",
    "        BVP_path = os.path.join(dir_path, subject_dirs[j], \"BVP.csv\")\n",
    "        EDA_path = os.path.join(dir_path, subject_dirs[j], \"EDA.csv\")\n",
    "        TEMP_path = os.path.join(dir_path, subject_dirs[j], \"TEMP.csv\")\n",
    "\n",
    "        # Read the CSV file\n",
    "        df_BVP = pd.read_csv(BVP_path, header=None)\n",
    "        df_EDA = pd.read_csv(EDA_path, header=None)\n",
    "        df_TEMP = pd.read_csv(TEMP_path, header=None)\n",
    "\n",
    "        # check whether the starting time are the same for 3 variables\n",
    "        if df_BVP.iloc[0,0] == df_EDA.iloc[0,0] and df_BVP.iloc[0,0] == df_TEMP.iloc[0,0]:\n",
    "            next\n",
    "        else:\n",
    "            print(0)\n",
    "\n",
    "        # found all starting time are the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "86c74aeb-8e96-4819-bf61-f7d773f2d4ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from above, found the starting time are the same for different variables for different subject\n",
    "# combine 3 variables and chunk them into 4 seconds with 1 second overlap\n",
    "for i in range(3):\n",
    "    dir_name = subdirs[i]\n",
    "    dir_path = os.path.join(base_path, dir_name)\n",
    "    subject_dirs = [name for name in os.listdir(dir_path) if os.path.isdir(os.path.join(dir_path, name))]\n",
    "    subject_dirs\n",
    "    for j in range(len(subject_dirs)):\n",
    "        subject_dirs[j]\n",
    "        os.listdir(os.path.join(dir_path, subject_dirs[j]))\n",
    "        BVP_path = os.path.join(dir_path, subject_dirs[j], \"BVP.csv\")\n",
    "        EDA_path = os.path.join(dir_path, subject_dirs[j], \"EDA.csv\")\n",
    "        TEMP_path = os.path.join(dir_path, subject_dirs[j], \"TEMP.csv\")\n",
    "\n",
    "        # Read the CSV file\n",
    "        df_BVP = pd.read_csv(BVP_path, header=None)\n",
    "        df_EDA = pd.read_csv(EDA_path, header=None)\n",
    "        df_TEMP = pd.read_csv(TEMP_path, header=None)\n",
    "\n",
    "        df_BVP = df_BVP.iloc[2:].reset_index(drop=True)\n",
    "        df_EDA = df_EDA.iloc[2:].reset_index(drop=True)\n",
    "        df_TEMP = df_TEMP.iloc[2:].reset_index(drop=True)\n",
    "\n",
    "        # downsample BVP to 32 hz\n",
    "        df_BVP = df_BVP.iloc[::2].reset_index(drop=True)\n",
    "        df_EDA = df_EDA.loc[df_EDA.index.repeat(8)].reset_index(drop=True)\n",
    "        df_TEMP = df_TEMP.loc[df_TEMP.index.repeat(8)].reset_index(drop=True)\n",
    "\n",
    "        # combine all three df into one data frame\n",
    "        # Step 1: Find the minimum number of rows\n",
    "        min_rows = min(len(df_BVP), len(df_EDA), len(df_TEMP))\n",
    "\n",
    "        # Step 2: Cut each dataframe to the minimum number of rows\n",
    "        df_BVP_cut = df_BVP.iloc[:min_rows].reset_index(drop=True)\n",
    "        df_EDA_cut = df_EDA.iloc[:min_rows].reset_index(drop=True)\n",
    "        df_TEMP_cut = df_TEMP.iloc[:min_rows].reset_index(drop=True)\n",
    "\n",
    "        # Step 3: combine columns\n",
    "        df= pd.concat([df_BVP_cut, df_EDA_cut, df_TEMP_cut], axis=1)\n",
    "\n",
    "        # assign column names\n",
    "        df.columns = ['BVP', 'EDA', 'TEMP']\n",
    "\n",
    "        # set index as a column named \"time\"\n",
    "        df = df.reset_index().rename(columns={'index': 'time'})\n",
    "        df['subject'] = subject_dirs[j]\n",
    "\n",
    "        # chunk df into chunks of 4 seconds with 1 second overlap\n",
    "\n",
    "\n",
    "        chunk_size = chunk_duration * fs\n",
    "        overlap_size = overlap_duration * fs\n",
    "        step_size = chunk_size - overlap_size  # How many rows to move forward for each new chunk\n",
    "\n",
    "        # Store all chunks\n",
    "        chunks = []\n",
    "\n",
    "        # Go through the DataFrame\n",
    "        for start in range(0, len(df) - chunk_size + 1, step_size):\n",
    "            chunk = df.iloc[start : start + chunk_size].reset_index(drop=True)\n",
    "            chunks.append(chunk)\n",
    "\n",
    "        # Now `chunks` is a list of DataFrames, each 128 rows, overlapping by 32 rows\n",
    "\n",
    "        chunks_all.append(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "1d30b009-7acd-4897-bcb3-60e32c20d9fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(chunks_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "b9748263-ca08-4098-a3bf-aab7216bc791",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, chunk in enumerate(chunks_all):\n",
    "    pd.concat(chunk, axis = 0).reset_index(drop = True).to_csv(f'chunk_{i}.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "41c01105-afc2-469f-a63d-aeb73ff4beac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\Ally\\\\Desktop\\\\sheng'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1983ffb-d584-45d4-9417-dc01bca81725",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
